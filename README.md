Home Credit Default Risk: Predicting Loan Defaults for Underbanked Populations

Project Overview
Built predictive models to help Home Credit assess loan default risk for people with little to no credit history. 
The goal is to improve default prediction while maintaining financial inclusion for underbanked populations.
Key Result: Random Forest model achieved 63% sensitivity in detecting defaults vs. 0% for baseline models, enabling Home Credit to prevent significant loan losses while still approving creditworthy applicants.

Business Problem
Home Credit provides consumer loans to people traditionally underserved by banks to those without traditional credit scores. Without credit bureau data, they struggle to predict which applicants will default, creating two risks:
â€¢	Approve bad loans = Defaults damage profitability
â€¢	Reject good applicants = Miss revenue and fail social mission
â€¢	Target Variable: Binary classification 91.9% repayment, 8.1% default due to highly imbalanced data
â€¢	Challenge: Build a model that maximizes default detection (sensitivity) while maintaining reasonable accuracy to avoid rejecting too many good customers.

Data Preparation
â€¢	Dataset: 307,511 loan applications with 220+ variables across 7 relational tables
â€¢	Feature Selection: Focused on 19 high-signal variables (income, employment, assets, credit history, demographics)
â€¢	Feature Engineering: Aggregated bureau credit data (average days since credit application - frequent applications signal risk)
â€¢	Class Imbalance: Applied downsampling to balance the highly skewed default rate (8.1%)

Models Developed
Model                  AUC         Sensitivity          Key Insight
Logistic Regression  0.666          ~0%                Too simple- became majority classifier
Random Forest        0.7190          .63               Best default detection
Neural Networks      0.651-0.695    0.16-0.34          Slow to train, poor sensitivity
 
 Selected Model: Random Forest with 5-fold CV and downsampling
Why Random Forest?
â€¢	63% sensitivity: Catches most potential defaults before approval
â€¢	71% specificity: Most good applicants still approved automatically
â€¢	Practical: Trains in 15 minutes (vs. 45+ for neural nets)
â€¢	Interpretable: Tree-based structure helps understand risk factors (regulatory requirement)

Results and Business Impact
Model Performance (Validation Set)
Confusion Matrix:
                  Predicted
Actual              No Default  Default
  No Default      39,754     16,330
  Default              1,843      3,575
Sensitivity:     63%   =   Catches 63% of defaults
Specificity:     71%   = Approves 71% of good loans
Overall Accuracy: 70%

What This Means for Home Credit
Before Model: Approve all loans = 8.1% default rate, significant losses
With Model:
Catch 3,575 of 5,418 defaults (63%) before approval
Prevent losses from bad loans that depends on average loan size
Trade-off: 16,330 good applicants flagged for manual review (acceptable cost vs. default losses)

Kaggle Competition Results
Public Score: 0.662 AUC
Private Score: 0.675 AUC
Scores within 1% = model generalizes well to unseen data

Key Technical Decisions
â€¢	Prioritized Sensitivity Over Accuracy
For Home Credit, missing a default (false negative) is more costly than flagging a good customer (false positive). Standard accuracy metrics (91%) are misleading with imbalanced data. The model could predict "no default" for everyone and be "accurate" but useless.
I understood that business priorities should drive metric selection. For lending, preventing defaults is more important than overall accuracy.
â€¢	Downsampling Beat Class Weighting
Tested multiple approaches to handle 91%-9% class imbalance:
Random Forest with downsampling: 63% sensitivity
Neural networks with 11.25x class weighting: 34% sensitivity
Threshold tuning alone: Didn't resolve underlying issue.
Downsampling the majority class during training balanced the model's learning while preserving the real distribution for evaluation.
â€¢	Simpler Model Outperformed Complex Ones
Despite testing neural networks with various architectures, Random Forest consistently performed better.I learned that proper data handling (downsampling) and domain-relevant features matter more than algorithm complexity. Also considered production constraints,Random Forest trains in 15 minutes vs. 45+ for neural nets.
â€¢	Feature Engineering from Multiple Tables
Aggregated bureau data to capture credit-seeking behavior.I joined multiple tables and engineered features that captured behavioral patterns not just static demographic data.

What I Would Do Differently
Given more time, I would:
â€¢	Use All 7 Tables: Currently used 2 of 7 available tables, payment history, previous applications, and installment data likely contain strong signals
â€¢	Advanced Imputation: Replace NA-dropping with mean/mode or k-NN imputation to retain more training data
â€¢	Model Ensembling: Stack Random Forest + XGBoost + LightGBM 
â€¢	Cost-Sensitive Learning: Assign explicit dollar costs to false negatives vs. false positives to optimize business profit directly

Technical Skills Demonstrated
â€¢	Machine Learning: Logistic Regression, Random Forest, Neural Networks (MLP)
â€¢	Imbalanced Data: Downsampling, class weighting, threshold tuning
â€¢	Model Evaluation: AUC-ROC, sensitivity/specificity, confusion matrices, 5-fold cross-validation
â€¢	Data Engineering: Multi-table joins, aggregation, feature selection
â€¢	R/tidyverse: caret, ranger, RWeka, dplyr, ggplot2
â€¢	Business Acumen: Chose metrics aligned with business costs, not just technical benchmarks

Questions: 
Walk me through your approach to this project."

Started with business understandingâ€”Home Credit needs to detect defaults while maintaining financial inclusion. Built baseline logistic regression (0% sensitivity due to imbalance), then Random Forest with downsampling (63% sensitivity). Tested neural networks extensively but they didn't justify the computational cost. Selected Random Forest for best sensitivity and practical deployment.

"How did you handle the class imbalance?"

Tried three approaches: downsampling (balanced training data), class weighting (weighted defaults 11.25x), and threshold tuning. Downsampling in Random Forest worked best, achieving 63% sensitivity vs. 34% with class weighting in neural nets.

"Why didn't neural networks perform better?"

With tabular data and limited feature engineering, tree-based methods often outperform deep learning. Also, training time was prohibitive (45+ minutes vs. 15 for Random Forest), and sensitivity remained poor despite extensive hyperparameter tuning.

"What would you improve with more time?"

Three priorities: (1) Feature engineeringâ€”create interaction terms and ratios, (2) Use all 7 data tables instead of just 2, (3) Ensemble multiple models like top Kaggle solutions. Could likely reach 0.75-0.80 AUC with these improvements.

"How does this translate to business value?"

If Home Credit processes 10,000 loans/month with 8% default rate, that's 800 defaults. Our model catches 504 of them (63%), preventing significant losses. Trade-off is flagging 1,633 good applicants for manual review, but cost of defaults far exceeds review costs.



Contact
Astha K C
ðŸ“§ asthakc.us@gmail.com| ðŸ’¼  https://www.linkedin.com/in/astha-k-c/|
Open to opportunities in Data Analytics, Business Analytics, Data Science, ML Engineering, and Risk Analytics.

Acknowledgments: This was a project for MSBA project. This repository contains my individual contributions: logistic regression model, Random Forest model development with downsampling and cross-validation, AUC evaluation framework, and model comparison analysis.
Course: MSBA Practice Project, University of Utah, Fall 2025
Last Updated: November 23, 2025

